\chapter*{Conclusion}\addcontentsline{toc}{chapter}{Conclusion}

This dissertation concerns \pzpdf s in the context of cosmology.
In \Chap{pedant}, I explore the mathematical underpinnings of \pzpdf s and identify the special assumptions necessary for the naive estimator of the redshift distribution function \Nz\ to be valid, conditions that generally do not and will not hold for current and future galaxy surveys.
In \Chap{chippr}, I propose and validate a Bayesian hierarchical model for the inference of the redshift density function \nz\ using \pzpdf s, presenting the \chippr\ public code.
In \Chap{pzdc1}, I stress-test the performance metrics that can be used to compare the myriad methods for obtaining \pzpdf s and isolate the impact of implicit priors on \pzpdf\ catalogs.
In \Chap{qp}, I investigate the optimal way to determine the storage parameterization of large \pzpdf\ catalogs under realistic computational resource constraints. 

\Chap{pedant} is a pedagogical text aimed at refreshing the principles of probability for an audience of astrophysicists.
Using only the most basic mathematical concepts, I address the question of how stacking could appear to derive a correct redshift distribution function \Nz\ when it is unsupported by the facts.
I then use a mathematically valid expression for the expected value of the redshift distribution function \Nz\ to identify the only circumstances under which stacking can recover the truth.
By showing that the assumptions required for stacking to be valid will not be satisfied by upcoming galaxy surveys like \lsst, I hope to put the archaic practice to rest, for good.
Though work remains to be done to change the way the average astronomer thinks about probability, \Chap{pedant} is at least a starting point for spreading knowledge.

\Chap{chippr} presents a method for using an arbitrary existing \pzpdf\ catalog for cosmological inference.
Though similar approaches have been used in the recent past, none both are applicable to catalogs that have already been produced by methods trusted by the \pz\ community and account explicitly for the role of priors in \pzpdf\ production.
The main challenge in using \chippr\ as the basis for a real cosmological analysis pipeline, say, for \lsst, is computational; \emcee\ is limited to $\leq 35$ parameters and the evaluation of the posterior PDF on \nz\ scales linearly with the number of galaxies in the catalog.
These obstacles may be overcome by using a different sampler and/or utilizing parallel processing, paths we will pursue in the coming years.

\Chap{pzdc1} is the culmination of three years of work by a team of over twenty researchers.
Though it began as a search for the ``best'' \pzpdf\ method, for inclusion in the \lsst\ pipeline, my contributions were the discovery that existing metrics of \pzpdf\ performance are inadequate for identifying techniques that serve the needs of the extragalactic astrophysics community and the isolation of the implict prior of each code due to the method and implementation rather than the training or template set \textemdash in short, the original plan could not be accomplished for deep, almost philosophical shortcomings in how cosmologists think about probability.
Plans for future work within \desc\ center around testing realistic incompleteness and inaccuracy of training and template sets, important goals for constraining the sensitivity of each method to realistic limitations on knowledge we will feed into the \desc\ pipeline.
However, I have also advocated for the creation of mock catalogs using a self-consistent forward model, facilitating access to true \pzpdf s that open the door to evaluation of \pzpdf\ techniques under more mathematically principled metrics.
In doing so, we may be able to obtain the implicit prior due to the method itself so that the \Chippr\ method may be applied to real data.

\Chap{qp} originated naturally from a known problem and a bet.
\lsst's public plan was to store one \pzpdf\ per galaxy in the form of $100$ evenly spaced floating point numbers for a redshift grid and $100$ floating point numbers for the PDF evaluated on that grid, which is obviously a tremendous waste of storage space if the redshift grid is shared among all galaxies and grossly inappropriate for \pzpdf s with narrow features, not to mention that it requires a choice of a single \pzpdf\ code in the absence of an obviously superior approach.
When Phil Marshall asserted that random samples should be stored instead, I countered with an assertion that quantiles, which store information equally in the space of probability density, would be guaranteed to be more efficient than samples, which store information uniform randomly in the space of probability density.
\qp\ was born out of a challenge for me to prove that my intuition was correct, and it resulted in a change for the better in \lsst's data management plan to make the most out of the resources allocated to the telescope.
While there are of course more complex possibilities, such as storing a different number of parameters for each \pzpdf\ or using quantiles that aren't spaced evenly, the default quantile solution still represents a substantial improvement over the default plan of evaluations on a shared, fine grid.
And, more importantly, the approach of \Chap{qp} can be applied to other science use cases to determine the optimal parameterization more generally.

\aim{Summarize my overall research program, how this work fits into the field as a whole, what broadly remains to be done, and my vision for where the field is going.
\begin{itemize}
	\item must be careful and correct, or we will only get out what we put in
	\item how we make decisions about what's good and what's bad makes a difference, must \textit{design} metrics appropriate to our goals even when inconvenient
\end{itemize}}

My research program is a multi-pronged effort to facilitate mathematically correct inference in cosmology using probabilistic data products appropriate for uncertainty-dominated data like those anticipated of upcoming galaxy surveys.
Beginning from a basic tenet of ``use every part of the animal,'' I have approached both problems that have been known to the cosmology community for years and those that had not yet been identified as such but nonetheless deserved solutions.
In producing the \qp\ and \chippr\ public codes as well as pedagogical texts, it is my hope that others can build on this work to perform valid inferences throughout extragalactic astrophysics, beyond the scale of problems I can aspire to solve myself.

