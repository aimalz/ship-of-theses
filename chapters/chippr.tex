\renewcommand{\chapid}{chippr}

% Chapter specific commands:
\newcommand{\Chippr}{\project{CHIPPR}}

% Math:
\newcommand{\nz}{$n(z)$}

\chapter{How to obtain the redshift distribution from probabilistic redshift estimates\chaplabel{chippr}}

This \paper\ represents joint work with [people] published in [place] as [citation].

\section{Chapter abstract}

A trustworthy estimate of the redshift distribution $n(z)$ is crucial for weak-lensing cosmology as we know it.
Spectroscopic redshifts for the dim and numerous galaxies of weak-lensing surveys are expected to be inaccessible, making photometric redshifts (photo-$z$s) the next-best alternative.
The nontrivial systematics affecting photo-$z$ estimation have motivated the weak-lensing community to favor photo-$z$ probability density functions (PDFs) as a more comprehensive alternative to photo-$z$ point estimates.
However, analytic methods for utilizing these new data products in cosmological inference are still evolving.
The ubiquitous methodology known as stacking produces a systematically biased estimator of $n(z)$ that worsens with decreasing signal-to-noise, the very regime where photo-$z$ PDFs are most necessary.
We introduce a mathematically rigorous probabilistic graphical model (PGM) of hierarchical inference of $n(z)$, which is provably the only self-consistent way to combine photo-$z$ PDFs to produce an estimator of $n(z)$.
The novel Cosmological Hierarchical Inference with Probabilistic Photometric Redshifts (CHIPPR) model yields a more accurate characterization of $n(z)$ by correctly propagating the redshift uncertainty information beyond the best-fit estimator produced by traditional procedures.
We conclude by propagating these effects to constraints in the space of cosmological parameters.

\section{Introduction}
\sectlabel{sec:intro}

\aim{Q: What is the status quo?\\
	A: The future of galaxy surveys is \pz s, and \pzpdf s rightfully replace \pz s; however, usage of \pzpdf s is limited.}

\aim{Q: Why should we question existing methods?\\
	A: Stacking is bad and only looks like it works because of assumptions that don't hold when the data is as bad as we anticipate.}

Though their potential to improve estimates of physical parameters is tremendous, photo-$z$ posterior probability distributions have been applied only to a limited extent.  
This paper aims to develop a clear methodology guiding the use of photo-$z$ PDFs in inference so they may best be utilized by the community.
They have been used to form selection criteria of samples from galaxy surveys without propagation 
through the calculations of physical parameters \citep{VanBreukelen2009,Viironen2015}.  
Probability cuts on Bayesian quantities are not uncommon \citep{Leung2015, DiPompeo2015a}, but that procedure does not fully take advantage of all information contained in a probability distribution for parameter inference.  

Despite the growing prevalence of photo-$z$ posterior production, no implementation of inference using photo-$z$ PDFs has yet been presented with a mathematically consistent methodology.  
We present and validate a technique for the use of photo-$z$ posterior distributions in inference of arbitrary statistics relevant to cosmology, large-scale structure, and galaxy evolution.  
For simplicity, we consider only one-point statistics, though future work will extend this methodology to higher-order statistics.

The redshift distribution function $N(z)$ serves as an ideal statistic upon which to demonstrate this novel approach.  
$N(z)$ is necessary for calculations of two-point correlation functions of weak gravitational lensing and counting statistics that are used to probe dark energy \citep{Masters2015}.  
$N(z)$ for observed galaxies has also been used to validate survey selection functions used in generation of realistic, multi-purpose mock catalogs \citep{Norberg2002}.  
Additionally, $N(z)$ has been the subject of inference using photo-$z$ probability distributions before \citep{Sheldon2012, Hildebrandt2012, Kelly2014, Benjamin2013, Bonnett2015a, Viironen2015, Asorey2016, Leistedt2016}, so comparisons to the literature may easily be made. 

\textbf{New paragraph here: Say what precision is needed for N(z) for future weak lensing surveys. Say what precision the mass function is needed (in, say cluster studies) for precision cosmology.}

\Sect{sec:meth} will derive the framework for exploring the full posterior of distribution for $N(z)$ using photo-$z$ probability distribution functions.  
\Sect{sec:exp} will describe how the model given in \Sect{sec:meth} is implemented.  
\Sect{sec:valid} will discuss the results of applying the fully probabilistic approach to mock and real datasets.

\begin{itemize}
	\item
	Q: How can we improve the effectiveness of using \pzpdf s in inference?\\
	A: Hierarchical inference is the only self-consistent way; see flow chart for outline of how it works in practice.
	\item
	Q: How does the result of \chippr compare to established estimators in terms of the accuracy of \nz ?\\
	A: \chippr\ yields the best possible \nz , conditional on the accuracy of the \pzpdf s used.
	\item
	Q: How does the result of \chippr compare to established estimators in terms of the precision on cosmological constraints?
\end{itemize}

\section{If stacking is wrong, why does it feel so right?}
\sectlabel{sec:pedantry}

The constraining power of current and upcoming telescope missions is contingent on our ability to obtain redshift estimates for large numbers of faint galaxies.
In the absence of spectroscopic redshifts, unreliable photometric redshift point estimates (photo-$z$s) have been superceded by photo-$z$ probability density functions (PDFs) that encapsulate their nontrivial uncertainties.
Initial applications of photo-$z$ PDFs in weak gravitational lensing studies of cosmology have employed naive methodologies for obtaining the redshift distribution function $N(z)$.
Though computationally straightforward, such techniques violate the laws of probability, triggering a proliferation of mathematically self-consistent models of varying complexity answering the question, ``What is the right way to obtain the redshift distribution function $N(z)$ from a catalog of photometric redshift PDFs?''
However, adoption of these principled models has been slow, perhaps due to a gap in understanding between those developing the methods and those applying them to real data.
This letter aims to bridge that gap by addressing the contrapositive of the more common presentation of such models, answering the question, ``Under what conditions do traditional stacking methods recover the true redshift distribution function $N(z)$?''

\subsection{Definitions}
\label{sec:def}

\begin{enumerate}
	\item state space $N(z)$ can take any value $N_{z} = 0, \dots, N_{tot}$, so $N_{tot}+1$ possibilities
	\item probability $n(z) \equiv p(z) = \frac{N_{z}}{N_{tot}}$, $\int n(z) dz = \int p(z) dz = 1$
	\item independent $p(z_{i} \cap z_{j}) = p(z_{i}) p(z_{j})$, not in general $(p(z_{i}) + p(z_{j})) / 2$
	\item disjoint $p(\sim z) = 1 - p(z)$, $p(z_{i} \cup z_{j}) = p(z_{i}) p(\sim z_{j}) + p(\sim z_{i}) p(z_{j}) + p(z_{i}) p(z_{j}) = p(z_{i})(1 - p(z_{j})) + (1 - p(z_{j}))p(z_{j}) + p(z_{i}) p(z_{j})$, also $p(z_{i} \cup z_{j}) = 1 - p(\sim z_{i} \cap \sim z_{j}) = 1 - p(\sim z_{i}) p(\sim z_{j}) = 1 - (1 - p(z_{i})) (1 -p(z_{j}))$
	\item expected value $\langle N_{z} \rangle = \frac{1}{N_{tot}+1} \sum_{N_{z} = 0}^{N_{tot}} N_{z} p(N_{z})$
	%; note that $\hat{n}(z) = \langle p(z) \rangle$ assuming $p(p(z)) = \mathcal{U}(0, 1)$
\end{enumerate}

galaxies have true redshifts in nature, so for every redshift $z$, there is a set $\mathbb{S}^{z}$ of $\textbf{N}_{z}$ galaxies that really do have redshift $z$\\
if we think there are $N_{z}$ galaxies with redshift $z$, there are $N_{z}!$ possible sets $k$ of galaxies that could lead to $N_{z}$ having redshift $z$\\
each such set $\mathbb{S}_{k}^{z}$ corresponds to exactly one corresponding set $\mathbb{S}_{k}^{\sim z}$ by the disjoint property of the state space

\subsection{Derivation}
\label{sec:pedanticmath}

proof by construction: the redshift distribution can be framed as an application of the binomial theorem

\subsubsection{A two-redshift universe of identical galaxies}

every photo-z PDF is the same and only two redshifts in the universe $\to$ flipping a single coin a finite number of times\\
every possible set $\mathbb{S}^{z}_{k}$ of $N_{z}$ galaxies with redshift $z$ corresponds to a set $\mathbb{S}^{\sim z}_{k}$ of $N_{tot}-N_{z}$ galaxies with redshift other than $z$\\
the binomial theorem states that $p(N_{z}) = \comb{N_{tot}}{N_{z}} \times p(z)^{N_{z}} \times (1-p(z))^{N_{tot}-N_{z}}$ for $\comb{N_{tot}}{N_{z}} \equiv \frac{N_{tot}!}{N_{z}! (N_{tot}-N_{z})!}$

\subsubsection{A many-redshift universe of identical galaxies}

every photo-z PDF is the same and many possible redshifts $\to$ the birthday problem\\
now let's consider a set $\mathbb{S}^{z}$ of $\textbf{N}_{z}$ galaxies with redshift $z$, which can be defined individually for all redshifts $z$\\
holding $\textbf{N}_{z}$ constant, there are $\mathbb{N}_{z}!$ possible combinations of galaxies in the set $\mathbb{S}^{z}$
the birthday problem is solved by $p(N_{z}) = 1 - p(z)^{N_{tot}} \times \perm{N_{tot}}{N_{z}}$ for $\perm{N_{tot}}{N_{z}} \equiv \frac{N_{tot}!}{(N_{tot}-N_{z})!} = N_{z}!\comb{N_{tot}}{N_{z}}$

\subsubsection{A many-redshift universe of different galaxies}

now consider many possible photo-z PDFs and many possible redshifts

Figure: isolating one redshift slice of $N(z)$

\begin{align}
p(N_{z}) &= %\sum_{permutation\ k}^{N_{tot}!} \left[ \prod_{i_{k} = 0}^{N_{z}} p(z_{i_{k}}) \prod_{j_{k} = N_{z} + 1}^{N_{tot}} 1 - p(z_{j_{k}}) \right]\\
\sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \prod_{i_{k} \in \mathbb{S}_{z}} p(z_{i_{k}}) \prod_{j_{k} \in \mathbb{S}_{\sim z}} 1 - p(z_{j_{k}}) \right]
\end{align}

Figure: posterior samples of $N_{z}$ for that slice

\subsection{Results}
\sectlabel{sec:res}

the stacked estimator of the redshift distribution yields a single redshift distribution function $\hat{N}_{z}$ rather than the probability distribution $p(N_{z})$, so here we'll use the reduction of \Eq{eq:complete} to its expected value $\langle N_{z} \rangle$

\begin{align}
\eqlabel{eq:complete}
\langle N_{z} \rangle &= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \prod_{i_{k} \in S_{z}} p(z_{i_{k}}) \prod_{j_{k} \in \mathbb{S}_{\sim z}} 1 - p(z_{j_{k}}) \right]
\end{align}

let's explore what assumptions must hold for \Eq{eq:complete} to be equivalent to \Eq{eq:stackwithdata}

\subsubsection{Perfectly informative data}
\sectlabel{sec:informative}

``$p(z)$'' is misleading, must be posterior conditioned on data $p(z \mid d_{i})$, and $p(z_{i})$ doesn't make sense because the domain of $z$ is the same for any galaxy $i$

\begin{align}
\label{eq:stackwithdata}
\hat{N}(z) &= \sum_{i = 0}^{N_{tot}} p(z \mid d_{i})
\end{align}

consider perfectly informative PDFs $p(z \mid d_{i}) = \delta(z, z_{i})$

\begin{align}
\label{eq:data}
\langle N_{z} \rangle &= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \prod_{i_{k} \in \mathbb{S}_{z}} p(z \mid d_{i_{k}}) \prod_{j_{k} \in \mathbb{S}_{\sim z}} 1 - p(z \mid d_{j_{k}}) \right]\\
&= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \prod_{i_{k} \in \mathbb{S}_{z}} \delta(z, z_{i_{k}}) \prod_{j_{k} \in \mathbb{S}_{\sim z}} 1 - \delta(z, z_{j_{k}}) \right]\\
&= \textbf{N}_{z} \left[ \prod_{i_{k} \in \mathbb{S}_{\textbf{N}_{z}}} \delta(z, z_{i_{k}}) \prod_{j_{k} \in S_{N_{tot} - \textbf{N}_{z}}} 1 - \delta(z, z_{j_{k}}) \right]\\
&= \textbf{N}_{z} \times (1 + 0)\\
\end{align}

every term is 0 except where $z=z_{jk}$, where it is 1, which only occurs for the true configuration is the PDFs are delta functions centered at the truth, so $\langle N_{z}\rangle = \hat{N}_{z}$, a histogram of spec-zs

what happens when we deviate away from this assumption of $p(z \mid d_{i}) = \delta(z, z_{i})$?

Figure:\\
left: rugplot, smoothing/histogram, recover $N(z)$\\
right: broadening of \zpdf s broadens $N(z)$

in realistic circumstances of photometry being noisy and less informative, stacking is guaranteed to yield an overly broad $N(z)$.

\subsubsection{Perfectly uninformative data}
\label{sec:prior}

if PDFs were solely dependent on data, $p(z \mid d_{i})$ would be same for all codes [CITE PZDC1], so they really must be implicit posteriors conditioned on data and assumptions $p(z \mid d_{i}, \tilde{N}_{z})$

\begin{align}
\label{eq:stackwithprior}
\hat{N}(z) &= \sum_{i = 0}^{N_{tot}} p(z \mid d_{i}, \tilde{N}_{z})
\end{align}

if the data is perfectly uninformative, $p(z \mid d, \tilde{N}_{z}) = \frac{\tilde{N}_{z}}{N_{tot}}$

\begin{align}
\label{eq:prior}
\langle N_{z} \rangle &= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \prod_{i_{k} \in S_{N_{z}}} p(z \mid d_{i_{k}}, \tilde{N}_{z}) \prod_{j_{k} \in S_{N_{tot} - N_{z}}} 1 - p(z \mid d_{j_{k}}, \tilde{N}_{z}) \right]\\
&= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \prod_{i_{k} \in S_{N_{z}}} \frac{\tilde{N}_{z}}{N_{tot}} \prod_{j_{k} \in S_{N_{tot} - N_{z}}} 1 - \frac{\tilde{N}_{z}}{N_{tot}} \right]\\
&= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \sum_{combination\ k}^{N_{tot}! / N_{z}! (N_{tot} - N_{z})!} \left[ \left(\frac{\tilde{N}_{z}}{N_{tot}}\right)^{N_{z}} \left(1 - \frac{\tilde{N}_{z}}{N_{tot}}\right)^{N_{tot}-N_{z}} \right]\\
&= \sum_{N_{z} = 0}^{N_{tot}} N_{z} \frac{N_{tot}!}{N_{z}! (N_{tot} - N_{z})!} \left[ \left(\frac{\tilde{N}_{z}}{N_{tot}}\right)^{N_{z}} \left(1 - \frac{\tilde{N}_{z}}{N_{tot}}\right)^{N_{tot}-N_{z}} \right]\\
\end{align}

this expression always reduces to the result of stacking

Figure: sum of same $N_{z}$ building up to that times $N_{tot}$

\subsection{Discussion}
\sectlabel{sec:disco}

i just showed when the right procedure is equivalent to stacking, but in that final case, it only approaches the true $\textbf{N}_{z}$ when $\tilde{n}(z) = \frac{\textbf{N}_{z}}{N_{tot}}$, i.e. when the truth is already known and baked into the PDFs.\\
if this were true, we would not need to take more data!

The conditions of \Sect{sec:informative} and \Sect{sec:prior} sort of occurred in the past but will never occur for ongoing and upcoming surveys, hence why we must not stack!

%\clearpage
\section{Method}
\sectlabel{sec:meth}

\aim{Q: How can we improve the effectiveness of using \pzpdf s in inference?\\
	A: Hierarchical inference is the only self-consistent way.}

Consider a survey of $J$ galaxies $j$, each with photometric data $\vec{d}_{j}$; thus the entire survey over some solid angle $\Omega$ produces the ensemble of photometric magnitudes (or colors) and their associated observational errors $\{\vec{d}_{j}\}$.  
Each galaxy $j$ has a redshift $z_{j}$ that we would like to learn; redshift is a parameter in this case.  
The distribution of the ensemble of redshifts $\{z_{j}\}$ may be described by the hyperparameters defining the redshift distribution function $N(z)$ that we would like to quantify.  
This situation may be considered to be a probabilistic generative model, illustrated by the directed acyclic graph of \Fig{fig:pgm}.  

\begin{figure}
	\vspace{0.5cm}
	\begin{center}
		\begin{tikzpicture}[node distance=1cm]
		
		\node (nz) [hyper] {$N(z)$};
		\node (z) [param, below of=nz,yshift=-0.25cm] {$z_{j}$};
		\node (mags) [data, below of=z,yshift=-0.25cm] {$\vec{d}_{j}$};
		\node (survey) [draw=black,fit={(mags.west)(z.north)(mags.south)(mags.east)}] {};
		\node [xshift=1.75cm,yshift=0.25cm] at (survey.south) {$j=1,\dots,J$};
		
		\draw [arrow] (nz) -- (z);
		\draw [arrow] (z) -- (mags);
		
		\end{tikzpicture}
		\caption{This directed acyclic graph illustrates the hierarchical model presented in this paper.  
			The redshift distribution function $N(z)$ exists independent of the survey of $J$ galaxies, indicated as a box.  
			The redshifts $\{z_{j}\}$ of all galaxies in the survey are parameters independently drawn from $N(z)$. 
			The photometric data $\vec{d}_{j}$ encircled in bold are observable quantities determined by the corresponding galaxy's redshift $z_{j}$.  
			Because the data $\vec{d}_{j}$ of each galaxy can be used to infer its redshift $z_{j}$, $N(z)$ may be inferred from the collection of $\{\vec{d}_{j}\}$.}
		\figlabel{fig:pgm}
	\end{center}
\end{figure}

The redshift distribution function 
\begin{equation}
\eqlabel{eq:distribution}
N(z)=dN/dz
\end{equation}
is the number of galaxies per unit redshift, effectively defining the evolution in the number of galaxies \citep{Menard2013}.  
In the following sections, we will present and compare methods for calculating $N(z)$ from photometric redshift probability distribution functions.  
\Sect{sec:prob} contains the mathematical derivation of a probabilistic model for $N(z)$ dependent on photo-$z$ probability distribution functions, and \Sect{sec:sheldon} contrasts the probabilistic model with alternative methods.

%\begin{eqnarray}
%\label{eq:distribution}
%N(z) &=& \frac{dN}{dz}
%\end{eqnarray}
%
%\clearpage
\subsection{Probabilistic Model Generalities}
\sectlabel{sec:prob}

%We begin by reframing the redshift distribution \nz\ from a probabilistic perspective.
%Here we define a redshift distribution \nz\ as the normalized probability density
%\begin{equation}
%\label{eqn:nz}
%\int_{-\infty}^{\infty}\ n(z)\ dz\ \equiv\ \frac{1}{J}\ \int_{-\infty}^{\infty}\ \sum_{j=1}^{J}\ \delta(z_{j},\ z)\ dz = 1
%\end{equation}
%of finding a galaxy $j$ in a catalog of $J$ galaxies having a redshift $z$.
%We may without loss of generality impose a parametrization
%\begin{equation}
%\label{eqn:fz}
%f_{\phi}(z)\ \equiv\ n(z)
%\end{equation}
%in terms of some parameter vector $\phi$.
%We believe that galaxy redshifts are indeed drawn from \nz, making it a probability density over redshift; this fact can also be confirmed from dimensional analysis of Equation~\ref{eqn:nz}.
%Therefore, it can be rewritten as
%\begin{equation}
%\label{eqn:pz}
%z_{j}\ \sim\ \Pr(z \mid \phi)\ \equiv\ f_{\phi}(z),
%\end{equation}
%a probability density over redshift conditioned on the parameters defining \nz.
%Note that $z_{j}$ does not depend on $z_{j'}$, a statement of the causal independence of galaxy redshifts from one another.
%
%In addition to believing \nz\ is a PDF from which redshifts are drawn, we also believe that there is some PDF from which photometric data $d$, which may be any combination of fluxes, magnitudes, colors, and their observational errors, are drawn.
%Such a PDF over data is a likelihood
%\begin{equation}
%\label{eqn:pzpdf}
%d\ \sim\ \Pr(d \mid z)
%\end{equation}
%conditioned on redshift.
%This assumption that the data are drawn from some function of the redshift forms the foundation upon which \pz\ estimation is based.
%Note that galaxies may have different observational data $d$ despite sharing the same redshift and that the data $d_{j}$ of one galaxy is causally independent of the redshifts $z_{j'}$ and data $d_{j'}$ of other galaxies.
%
%This description of the physical system corresponds to a forward model by which we actually believe photometry is generated:
%\begin{enumerate}
%	\item There exists a redshift distribution \nz\ with parameters $\phi$.
%	\item Galaxy redshifts $\{z_{j}\}$ are independent draws from $\Pr(z \mid \phi)$.
%	\item Galaxy photometry $d_{j}$ is drawn from the likelihoods $\Pr(d | z_{j})$.
%\end{enumerate}
%A forward model such as this corresponds to a probabilistic graphical model (PGM), represented by a directed acyclic graph (DAG) as in Figure~\ref{fig:pgm}.
%\begin{figure}
%	\begin{center}
%		\includegraphics[width=0.25\textwidth]{fig/pgm.png}
%		\caption{The PGM}
%		\label{fig:pgm}
%	\end{center}
%\end{figure}
%A DAG conveys the causal relationships between physical parameters and, like a Feynman diagram in the context of particle physics, is a shorthand for mathematical relationships between variables.

We begin by parametrizing $N(z)$ in terms of $\vec{\theta}$, comprising some set of hyperparameters that define the form $N(z)$ may take in whatever basis we choose.  
At this point, these hyperparameters are quite general and may represent coefficients in a high-order polynomial as a function of redshift, a set of means and variances defining Gaussians that sum to the desired distribution, a set of histogram heights that describe a binned version of the redshift distribution function, etc.  
We define a function $f_{\vec{\theta}}(z)=N(z)$ that transforms these hyperparameters into the redshift distribution function $N(z)$.  
Because 
\begin{equation}
\eqlabel{eq:definition}
N(z)\propto p(z|\vec{\theta}),
\end{equation}
we may discontinue discussion of $N(z)$ in favor of the likelihood $p(z|\vec{\theta})$.

In this paper, we shall work exclusively with log-probabilities.  
What we wish to estimate is the full log-posterior probability distribution (hereafter the full log-posterior) of the hyperparameters $\vec{\theta}$ given the data $\{\vec{d}_{j}\}$.  
By Bayes' Rule, the full log-posterior $\ln[p(\vec{\theta}|\{\vec{d}_{j}\})]$ may be expressed in terms of the full log-likelihood probability distribution (hereafter the full log-likelihood) $\ln[p(\{\vec{d}_{j}\}|\vec{\theta})]$ by way of a hyperprior log-probability distribution (hereafter the hyperprior) $\ln p(\vec{\theta})$ over the hyperparameters and the log-probability of the data $\ln[p(\{\vec{d}_{j}\})]$.  
The hyperprior expresses our beliefs about the distribution of the hyperparameters comprising $\vec{\theta}$.  
This matter is a choice we cannot avoid making and one which is often inspired by the results of previous galaxy surveys.  
The hyperprior chosen here will be elaborated upon in \Sect{sec:exp}.

The full log-posterior can be probed with a quantity proportional to it so the log-probability of the data is never explicitly necessary.  
The full log-likelihood may be expanded in terms of a marginalization over the redshifts as parameters, as in 
\begin{equation}
\eqlabel{eq:marginalize}
\ln[p(\{\vec{d}_{j}\}|\vec{\theta})] = \ln\left[\int\ p(\{\vec{d}_{j}\}|\{z_{j}\})\ p(\{z_{j}\}|\vec{\theta})\ d\{z_{j}\}\right].
\end{equation}

We shall make two assumptions of independence in order to make the problem tractable; their limitations are be discussed below.  
First, we take $\ln[p(\{\vec{d}_{j}\}|\{z_{j}\})]$ to be the sum of $J$ individual log-likelihood distribution functions $\ln[p(\vec{d}_{j}|z_{j})]$, as in 
\begin{equation}
\eqlabel{eq:indiedat}
\ln[p(\{\vec{d}_{j}\}|\{z_{j}\})] = \sum_{j=1}^{J}\ \ln[p(\vec{d}_{j}|z_{j})],
\end{equation}
a result of the definition of probabilistic independence.  
Second, we shall assume the true redshifts $\{z_{j}\}$ are $J$ independent draws from the true $p(z|\vec{\theta})$.  
Additionally, $J$ itself is a Poisson random variable.  
The combination of these assumptions is given by 
\begin{equation}
\eqlabel{eq:indie}
\ln[p(\{z_{j}\}|\vec{\theta})] = -\int\ f_{\vec{\theta}}(z)\ dz + \sum_{j=1}^{J}\ \ln[p(z_{j}|\vec{\theta})].
\end{equation}
It is important to note that the integral $\int N(z)\ dz$ is not constrained to equal the variable defining the Poisson distribution but instead $J$ by \Eq{eq:definition}, which can be thought of as another parameter.  
A detailed discussion of this matter may be found in \citet{Foreman-Mackey2014}.  
Applying Bayes' Rule, we may combine terms to obtain 
\begin{equation}
\eqlabel{eq:posterior}
\ln[p(\vec{\theta}|\{\vec{d}_{j}\})] \propto \ln[p(\vec{\theta})]\ -\int f_{\vec{\theta}}(z)\ dz + \sum_{j=1}^{J}\ \ln\left[\int\ p(\vec{d}_{j}|z_{j})\ p(z_{j}|\vec{\theta})\ dz_{j}\right].
\end{equation}

\Eq{eq:posterior} contains two quantities that merit further discussion, the prior distribution $p(\vec{\theta})$ discussed further in \Sect{sec:exp} and the photo-$z$ log-likelihoods $\ln[p(\vec{d}_{j}|z_{j})]$ that have not been mentioned since \Eq{eq:marginalize}.  
Though photo-$z$ log-likelihoods would be desirable for use in these equations, they are not generally the product of either empirical and data-driven methods for obtaining photo-$z$ probability distributions.  
Though probabilistic photo-$z$s are typically reported as generic probability distributions $p(z_{j})$, the methods that produce them may be understood to always yield posteriors, probability distributions conditioned on the data we believe to be true.  
If they were not based in this assumption, they would require a sum over an infinite space of possible datasets.

Posteriors differ from likelihoods by way of a prior distribution, so we cannot simply assume that the available data products are photo-$z$ posteriors $p(z_{j}|\vec{d}_{j})$.  
Rather, we consider the published catalog of $p(z_{j})$ interim photo-$z$ posteriors $p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})$.  
There must have been some interim prior probability distribution $p(z|\vec{\theta}^{*})$ defined in terms of the interim prior parameter values (hereafter the interim prior) $\vec{\theta}^{*}$ explicitly chosen or implicitly made to perform the calculation of the probabilistic photo-$z$s.  
If it is implicit, it may not be representable in the parametrization we have chosen, and furthermore it may not be known at all; a method that produces interim photo-$z$ posteriors of this kind is not suitable for inference.  
However, so long as the interim prior is known, hierarchical inference is possible. 

The interim prior $\vec{\theta}^{*}$ may be thought of as an initial guess for the parameters contained in $\vec{\theta}$, inspired by the generative model for photometry from the redshift distribution functions and including some parameters defining intrinsic galaxy spectra and instrumental effects. 
(See \citealt{Benitez2000} for more detail.)  
For statistical purposes, we would like any interim prior to be uninformative, but this is rarely achievable.  
In the case of estimating $N(z)$ photometrically, it is common to use $\vec{\theta}^{*}$ corresponding to $N(z)$ derived from some different, spectroscopically confirmed sample or from a cosmological simulation.

Since we only have access to interim photo-$z$ posteriors, we must be able to write the full log-posterior in terms of interim photo-$z$ log-posteriors rather than the log-likelihoods of \Eq{eq:posterior}.  However, we will need an explicit statement of this interim prior for whatever method is chosen to produce the interim photo-$z$ posteriors.  
To perform the necessary transformation from likelihoods to posteriors, we follow the reasoning of \citet{Foreman-Mackey2014}.  
Let us consider the probability of the parameters conditioned on the data and an interim prior and rewrite the problematic likelihood of \Eq{eq:posterior} as 
\begin{equation}
\eqlabel{eq:trick}
p(\vec{d}_{j}|z_{j}) = p(\vec{d}_{j}|z_{j})\ \frac{p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})}{p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})}.
\end{equation}

Once the interim prior $\vec{\theta}^{*}$ is explicitly introduced, we may expand the denominator according to Bayes' Rule to get 
\begin{equation}
\eqlabel{eq:expand}
p(\vec{d}_{j}|z_{j}) = p(\vec{d}_{j}|z_{j})\ p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})\ \frac{p(\vec{d}_{j}|\vec{\theta}^{*})}{p(z_{j}|\vec{\theta}^{*})\ p(\vec{d}_{j}|z_{j},\vec{\theta}^{*})}.
\end{equation}
Because there is no direct dependence of the data upon the hyperparameters, we may again expand the term $p(\vec{d}_{j}|z_{j},\vec{\theta}^{*})$ to obtain 
\begin{equation}
\eqlabel{eq:indterm}
p(\vec{d}_{j}|z_{j}) = p(\vec{d}_{j}|z_{j})\ p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})\ \frac{p(\vec{d}_{j}|\vec{\theta}^{*})}{p(z_{j}|\vec{\theta}^{*})\ p(\vec{d}_{j}|z_{j})\ p(\vec{d}_{j}|\vec{\theta}^{*})}.
\end{equation}
Canceling the undesirable likelihood terms $p(\vec{d}_{j}|z_{j})$ and $p(\vec{d}_{j}|\vec{\theta}^{*})$ yields
\begin{equation}
\eqlabel{eq:cancel}
p(\vec{d}_{j}|z_{j}) = \frac{p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})}{p(z_{j}|\vec{\theta}^{*})}.
\end{equation}
We put this all together to get the full log-posterior probability distribution of 
\begin{equation}
\eqlabel{eq:final}
\ln[p(\vec{\theta}|\{\vec{d}_{j}\})] \propto \ln[p(\vec{\theta})]-\int\ f_{\vec{\theta}}(z)\ dz + \sum_{j=1}^{J}\ \ln\left[\int\ p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})\ \frac{p(z_{j}|\vec{\theta})}{p(z_{j}|\vec{\theta}^{*})}\ dz_{j}\right].
\end{equation}

The argument of the integral in the log-posterior of \Eq{eq:final} depends solely on knowable quantities (and those we must assume) and can be calculated for a given set of interim photo-$z$ log-posteriors $\{\ln[p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})]\}$ and the interim prior $p(z|\vec{\theta}^{*})$ upon which their determination was based, noting the relation of 
\begin{equation}
\eqlabel{eq:params}
p(z_{j}|\vec{\theta}) = \frac{f_{\vec{\theta}}(z_{j})}{\int\ f_{\vec{\theta}}(z_{j})\ dz_{j}}.
\end{equation}
Since we cannot know constant of proportionality, we sample the desired full log-posterior $\ln[p(\vec{\theta}|\{\vec{d}_{j}\})]$ using Monte Carlo-Markov chain (MCMC) methods.  
The method outlined here is valid regardless of how the interim photo-$z$ log-posteriors are calculated so the many approaches to producing photo-$z$ probability distributions will not be discussed; though the matter is outside the scope of this paper, reviews of various methods have been presented in the literature \citep{Sheldon2012, Ball2008, CarrascoKind2013, CarrascoKind2014a}.

To be clear, the following assumptions must be made in order to apply this method:

\begin{enumerate}
	\item Photometric measurements of galaxies are independent Poisson draws from the set of all galaxies such that \Eq{eq:indiedat} and \Eq{eq:indie} hold.
	\item We take the reported interim photo-$z$ posteriors to be accurate estimates of the true photo-$z$ posteriors and assume we are given the interim prior used to produce them.
	\item We must assume a log-hyperprior constraining the underlying probability distribution of the hyperparameters, which is informed by our prior beliefs about the true redshift distribution function.
\end{enumerate}

These assumptions have known limitations.  
First, the photometric data are not a set of independent measurements; the data are correlated not only by the conditions of the experiment under which they were observed but also by redshift covariances resulting from physical processes governing underlying galaxy spectra and their relation to the redshift distribution function.  
Second, the reported interim photo-$z$ posterior distributions may not be trustworthy; there is not yet agreement on the best technique to obtain photo-$z$ probability distributions, and the interim prior may not be appropriate or even known to us as consumers of interim photo-$z$ posteriors.  
Third, the hyperprior may be quite arbitrary and poorly motivated if the underlying physics is complex, and it can only be appropriate if our prior beliefs about the redshift distribution function are accurate.

\subsection{Application to cosmology}
\sectlabel{sec:application}

The problem facing cosmologists is to determine the true parameters $\phi_{0}$ of \nz\ from observing the photometry $\{d_{j}\}$ of a large sample of galaxies $j$.
To self-consistently propagate the uncertainty in the redshifts, however, it is more appropriate to estimate the posterior $\Pr(\phi \mid \{d_{j}\})$ over all possible parameters $\phi$ (and thus potential redshift distributions \nz ) conditioned on all the observed data $\{d_{j}\}$ available in a generic catalog.

In order to use the DAG of \Fig{fig:pgm} to derive an expression for $\Pr(\phi \mid \{d_{j}\})$ in terms of \pzpdf s, we must introduce two more concepts, confusingly named the implicit prior and the prior probability density.

When we constrain the redshift of a galaxy using its observed photometric data $d_{j}$, we are effectively estimating a posterior $\Pr(z \mid d_{j})$.
However, to do this, we must have a model for the general relationship between redshifts and photometry, whether empirical, as is the case for machine learning \pzpdf\ methods, or analytic, as is the case for template-based \pzpdf\ methods.
Such a relationship is defined in the space of probability density over redshift, so it must be able to be parameterized by the same functional form $f_{\phi}(z)$ as \nz .
It is thus natural to write it as $\Pr(z \mid \phi^{*})$, where $\phi^{*}$ is the parameters for this relationship under some generic \pzpdf\ method.
We call $\Pr(z \mid \phi^{*})$ the \textit{implicit prior}, as it is rarely explicitly known nor chosen by the researcher; for template-based methods where it can be chosen to be ``realistic,'' it may be appropriate to call it an \textit{interim prior}.
Because the implicit prior is unavoidable, the \pzpdf s reported by any method are really \textit{implicit-weighted posteriors} $\Pr(z \mid d, \phi^{*})$.

The prior probability density $\Pr(\phi)$ is a more familiar concept in astronomy; to progress, we will have to choose a prior probability density over all possible parameters $\phi$.
This prior need not be excessively proscriptive; for example, it may be chosen to enforce smoothness at physically motivated scales in redshift without imposing any particular region as over- or under-dense.

With these definitions, we obtain the desired expression for $\Pr(\phi \mid \{d_{j}\})$,
\begin{align}
\begin{split}
\eqlabel{eqn:fullpost}
\ln[\Pr(\phi \mid \{d_{j}\})] & \propto \ln[\Pr(\phi)] + \ln \left[\int dz \right.\\
& \left. \exp \left[\sum_{j=1}^{J} \left(\ln[\Pr(z \mid d_{j}, \phi^{*})] \right. \right. \right.\\
& \left. \left. \left. + \ln[\Pr(z \mid \phi)] - \ln[\Pr(z \mid \phi^{*})]\ \right)\ \right]\ \right] ,
\end{split}
\end{align}
which is the heart of \chippr.
The entire derivation of \Eq{eqn:fullpost} is provided in \Sect{app:math}.

%\clearpage
\subsection{Alternative Approaches}
\sectlabel{sec:sheldon}

It will be desirable to compare the result of this method to the estimates of the hyperparameters obtained by two popular alternatives used in the literature, known as stacking and point estimation.   
These have been compared 
to one another by \citet{Hildebrandt2012}, \citet{Benjamin2013}, and \citet{Asorey2016}.

Stacking directly calculates the full posterior for the entire dataset using the interim photo-$z$ posteriors for each galaxy according to 
\begin{equation}
\eqlabel{eq:stack}
f_{\hat{\theta}}(z) = \sum_{j=1}^{J}\ p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})
\end{equation}
\citep{Lima2008}.  
Stacking is considered the preferred method for obtaining $N(z)$ from a dataset of interim photo-$z$ posteriors \citep{Sheldon2012, Kelly2014, Benjamin2013, Bonnett2015a, Viironen2015, Asorey2016}.  
However, it must be noted here that \Eq{eq:stack} is not in general mathematically valid.  
(See \citet{Hogg2012} for a complete discussion.)  
The estimator produced by stacking shall henceforth be referred to as the ``stacked'' estimator.

Point estimation converts the interim photo-$z$ posteriors $p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})$ into delta functions with all probability at a single estimated redshift.  
Some variants of point estimation choose this single redshift to be that of maximum a posteriori probability $argmax[p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})]$ or the expected value of redshift $E[z]=\int z_{j}\ p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})\ dz_{j}$.  
Stacking these modified interim photo-$z$ posteriors leads to the marginalized maximum a posteriori (MMAP) estimator and the marginalized expected value (MExp) estimator.

It is worth discussing the relationship between point estimation and stacking.  
When the point estimator of redshift is equal to the true redshift, stacking delta function photo-$z$ posteriors will indeed lead to an accurate recovery of the true redshift distribution function.  
However, stacking is in general applied indiscriminately to broader photo-$z$ posteriors, and point estimators of redshift are not in general perfectly accurate.  
It is for these reasons that alternatives are considered here.

A final estimator of the hyperparameters is the maximum marginalized likelihood estimator (MMLE), the value of $\hat{\theta}$ maximizing the log posterior given by \Eq{eq:final} using any optimization code.  
To compare with sampling, the MMLE also depends on the choice of the hyperprior distribution, and it does not produce a full posterior probability distribution over the 
parameters of interest, only point estimators.  
It must be noted that computation of the MMLE may be unstable depending on the strengths and weaknesses of the optimizer.  
In general, derivatives will not be available for the full posterior distribution, restricting optimization methods used.
%\begin{equation}
%\label{eq:mmle}
%\ln[p(\{\vec{d}_{j}\}|\vec{\theta})] \propto -\int\ f_{\vec{\theta}}(z)\ 
%dz+\sum_{j=1}^{J}\ln\left[\int\ 
%\exp\left[\ln[p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})]+\ln[f_{\vec{\theta}}(z)]-\l
%n[f_{\vec{\theta}^{*}}(z)]\right]\ dz\right],
%\end{equation}
%accessible with any optimization code.

%\clearpage
\subsection{Implementation}
\sectlabel{sec:exp}

We ran several tests of this approach to demonstrate its usage and validity using a procedure outlined in this section.  
In \Sect{sec:alldata} we describe the method by which interim photo-$z$ log-posteriors $\{\ln[p(z_{j}|\vec{d}_{j})]\}_{J}$ are synthesized as well as the real datasets considered for comparison.  
In \Sect{sec:mcmc} we describe the algorithm used to compute the full log-posterior distribution $\ln[p(\vec{\theta}|\{\vec{d}_{j}\})]$.  
In \Sect{sec:diag} we outline the measures used to evaluate the performance of the method.

%\clearpage
\subsubsection{Code}
\sectlabel{sec:mcmc}

In this study, we compare the results of \Eq{eqn:fullpost} to those of the two most common approaches to estimating \nz\ from a catalog of \pzpdf s: the distribution $n(z_{\mathrm{max}})$ of the redshifts at maximum posterior probability (i.e. the modes of the \pzpdf s), and the stacked estimator
\begin{align}
\eqlabel{eqn:stacked}
\hat{n}(z) &\equiv \frac{1}{J}\ \sum_{j=1}^{J}\ \Pr(z \mid d_{j}, \phi^{*}) .
\end{align}

We perform this comparison on mock data in the form of catalogs of emulated \pzpdf s generated via the forward model discussed above and presented in detail in \Sect{sec:data}.
The mock data emulates the three sources of error of highest concern to the \pz\ community: intrinsic scatter, catastrophic outliers, and systematic bias.
\Fig{fig:mega_scatter} illustrates these three effects individually at ten times the tolerance of the upcoming Large Synoptic Survey Telescope (LSST).
\begin{figure*}
	\begin{center}
		\includegraphics[width=0.3\textwidth]{figures/chippr/hivarsigmas_scatter.png}
		\includegraphics[width=0.3\textwidth]{figures/chippr/uouthi_scatter.png}
		\includegraphics[width=0.3\textwidth]{figures/chippr/neghivarbias_scatter.png}
		\caption{intrinsic scatter (left), uniform outliers (center), bias (right);
			Not sure how best to broach the subject of nonuniform outliers and bias as model misspecification
		}
		\figlabel{fig:mega_scatter}
	\end{center}
\end{figure*}
Tests including all three effects at the tolerance levels of LSST are presented in \Sect{sec:results}.

The testing procedure is implemented in \texttt{Python} and is made public as Cosmological Hierarchical Inference with Probabilistic Photometric Redshifts (CHIPPR).  
The code takes as input a \texttt{csv} file containing the basis for the binning of redshift space, a specification of the interim prior $\vec{\theta}^{*}$, and a catalog of $J$ interim photo-$z$ log-posteriors $\{\ln[p(z_{j}|\vec{d}_{j},\vec{\theta}^{*})]\}_{J}$ in the step-function basis.

The \texttt{emcee} \citep{Foreman-Mackey2013} implementation of ensemble sampling is applied to sample the full log-posterior of (\ref{eq:final}).   
For each of some $W$ walkers at each iteration $i$, a proposal distribution $\vec{\theta}^{i}$ generated from the hyperprior distribution and evaluated for acceptance to or rejection from the desired log-posterior distribution.  
Two threshold conditions are defined, one designating all previous samples to be ignored as as products of a "burn-in" phase and another indicating when a sufficient number of "post-burn" samples have been accepted.  
In this case, the first threshold (described in \Sect{sec:acorr}) is defined in terms of sub-runs of $I_{0}=10^{3}$ accepted samples, and the second is defined as an accumulation of $I_{tot}=10^{4}$ samples.

The sampler is initialized with $M=100$ walkers each with a value chosen from a Gaussian distribution of identity covariance around a sample from the hyperprior distribution.  
An example of such samples from the prior are shown in \Fig{fig:prior}.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/null_priorsamps.pdf}
	\caption{This figure shows samples (colored lines) of $p(z|\vec{\theta})$ where each $\vec{\theta}$ is drawn from the hyperprior distribution $p(\vec{\theta})$ defined in \Sect{sec:exp} with \Eq{eq:priorcov}.}
	\label{fig:prior}
\end{figure}

The input/output format chosen for this work is \texttt{HDF5} because of its efficiency for large amounts of data.  
The resulting output is a set of $I$ ordered \texttt{hickle} files enumerated by $\rho$ containing the state information after each sub-run.  
The state information includes $\frac{I_{0}}{s}$ actual samples $\vec{\theta}_{i}$ for a pre-specified chain thinning factor $s$ and their full posterior probabilities $p(\vec{\theta}_{i}|\{\vec{d}_{j}\})$ as well as the autocorrelation times and acceptance fractions calculated for each element of $\vec{\theta}$ over the entire sub-run.  

%\clearpage
\subsubsection{Convergence Criteria}
\sectlabel{sec:acorr}

In addition to qualitative visual inspection of the chains, two quantities that probe the convergence of the sampler are used in this study, the autocorrelation time and the Gelman-Rubin convergence criterion.  
%\Fig{fig:chains} shows the %evolution of the values of one parameter of one walker over the course of all %iterations of the sampler.

%\begin{figure}
%%\includegraphics[width=0.5\textwidth]{figs/null/chain0.pdf}
%\caption{This figure shows the evolution of one walker's parameter values for 
%one element of the parameter vector $\vec{\theta}$ as a function of iteration 
%number, demonstrating the completion of the burn-in phase.}
%\label{fig:chains}
%\end{figure}

The autocorrelation time is effectively a measure of the efficiency of the method and can be described as the expected number of iterations necessary to accept a new sample independent of the current accepted sample.  
A sampler that converges faster will have a smaller autocorrelation time, and smaller autocorrelation times are preferable because it means fewer iterations are wasted on non-independent samples when independent samples are desired.  
See \citet{Foreman-Mackey2013} for a more complete exploration of the autocorrelation time.  
In all tests discussed here, autocorrelation times across walkers and parameters were approximately 20, meaning two samples 20 or more iterations apart were independent, a satisfactory level of efficiency.  
Low autocorrelation times are a necessary but not always sufficient convergence condition, as the autocorrelation times calculated for tests in this paper were constant across all sub-runs, even those that were obviously burning in.  

The Gelman-Rubin statistic
\begin{equation}
\eqlabel{eq:gr}
R_{k} = \sqrt{\frac{(1-\frac{2}{I_{0}})w_{k}+\frac{2}{I_{0}}b_{k}}{w_{k}}},
\end{equation}
a weighted sum of the mean $w_{k}$ of the variances within individual walkers' chains and the variance $b_{k}$ between chains of different walkers $m$, is calculated over each sub-run $i$ to determine the duration of the burn-in period.  
Convergence is achieved when the statistic approaches unity.  

%\clearpage
\subsubsection{Diagnostics}
\sectlabel{sec:diag}

The results of the computation described in \Sect{sec:mcmc} are evaluated for accuracy on the basis of some quantitative measures.  
Beyond visual inspection of samples, we calculate summary statistics to quantitatively compare different estimators' precision and accuracy.  
Since MCMC samples of hyperparameters are Gaussian distributions, we can quantify the breadth of the distribution for each hyperparameter using the standard deviation regardless of whether the true values are known.  

In simulated cases where the true parameter values are known, we calculate the Kullback-Leibler divergence (KLD), given by 
\begin{equation}
\eqlabel{eq:kl}
KL_{\dagger,\ddagger} = \int\ p(z|\theta^{\dagger})\ \ln\left[\frac{p(z|\theta^{\dagger})}{p(z|\theta^{\ddagger})}\right]\ dz,
\end{equation}
which measures a distance between parameter values $\vec{\theta}^{\dagger}$ and $\vec{\theta}^{\ddagger}$ that is invariant under changes of variables.  
We note that $KL_{\dagger,\ddagger}\neq KL_{\ddagger,\dagger}$, so both must be calculated, and the minimum of the two is reported in this study.  
In simulated tests, $\vec{\theta}^{\ddagger}$ is the true value and $\vec{\theta}^{\dagger}$ is the value produced by one of the methods in question.  

\subsection{Limitations}
\sectlabel{sec:limitations}

Finally, we explicitly review the assumptions made by this approach.
\begin{itemize}
	\item prior $\Pr(\phi)$
	\item implicit prior $\phi^{*}$ known
	\item \pzpdf s are accurate posteriors
	%draws from photo-$z$ PDFs do not match marginal distributions in $z_true$ vs. $z_phot$ space, i.e. toy model with $z_phot$ drawn from an uglier space but pdfs evaluated in lsst-limits space
\end{itemize}



\subsection{Data}
\sectlabel{sec:alldata}

The parametrization of $N(z)$ chosen here is a step function formulation according to
\begin{eqnarray}
\eqlabel{eq:steps}
f_{\vec{\theta}}(z)&=&\sum_{k=1}^{K}\exp[\theta_{k}]s(z;k)\\
s(z;k)&=&\left\{\begin{array}{ccc}0&\text{for}&z<z_{k-1}\\(z_{k}-z_{k-1})^{-1} \equiv\Delta_{k}&\text{for}&z_{k-1}<z<z_{k}\\0&\text{for}&z_{k}<z\end{array}\right
\}.
\end{eqnarray}
All tests in this paper will be conducted with $z_{0}=0.0$, $z_{K}=1.1$, and $K=35$, the endpoints and dimensionality of the published BOSS DR8 photo-$z$ interim posteriors \citet{Sheldon2012}.  
We also define $\bar{z}_{k}\equiv(z_{k}+z_{k-1})/2$.

The hyperprior distribution chosen here is a multivariate normal distribution with mean $\vec{\mu}$ equal to the interim prior $\vec{\theta}^{*}$ and covariance
\begin{equation}
\eqlabel{eq:priorcov}
\Sigma_{k,k'} = q\ \exp[-\frac{e}{2}\ (\bar{z}_{k}-\bar{z}_{k'})^{2}]\ +\ t\delta(k,k')
\end{equation}
inspired by one used in Gaussian processes, where $k$ and $k'$ are indices ranging from $1$ to $K$ and $q=1.0$, $e=100.0$, and $t=q\cdot10^{-5}$ are constants chosen to permit draws from this prior distribution to produce shapes similar to that of a true $\tilde{\theta}$.  
We adapt the full log-posterior of \Eq{eq:final} to the binning of redshift space chosen in \Sect{sec:mock}.

In the following sections we motivate several informative tests, summarized in \Tab{tab:key}.  
The code was tested on simulated datasets each of size $J'=10,000$.  
The fiducial experiment of \Sect{sec:mock} is outlined first. 
Seven other cases vary the shapes of the photo-$z$ likelihoods (\Sect{sec:imprecision} and \Sect{sec:inaccuracy}), the true redshift distribution function (\Sect{sec:fake-data}), and the interim prior (\Sect{sec:interim-data}).  
Two additional cases with SDSS-III BOSS DR10 data described in \Sect{sec:data} are also considered.

\begin{table}
	\begin{tabular}{llll}
		\textul{Title} & \textul{True $N(z)$} & \textul{Interim Prior} & 
		\textul{Photo-$z$ PDFs}\\
		Fiducial & Physically motivated, & Uniform & Single Gaussians\\
		& featured $N(z)$ &&\\
		Precise & Physically motivated, & Uniform & Single, Narrow Gaussians\\
		& featured $N(z)$ &&\\
		Imprecise & Physically motivated, & Uniform & Single, Broad Gaussians\\
		& featured $N(z)$ &&\\
		Trending & Physically motivated, & Uniform & Single Gaussians\\
		& featured $N(z)$ && with $\sigma_{j}\sim z^{*}_{j}$\\
		Multimodal & Physically motivated, & Uniform & Multiple Gaussians\\
		& featured $N(z)$ &&\\
		Sampled & Physically motivated, & Uniform & Sampled, Single Gaussian\\
		& featured $N(z)$ &&\\
		Featured & Single, Narrow Gaussian & Uniform & Single, Broad Gaussians\\
		Unimodal $\vec{\theta}_{0}$ & Physically motivated, & Low-$z$ Favoring & 
		Single, Broad Gaussians\\
		& featured $N(z)$ &&\\
		Bimodal $\vec{\theta}_{0}$& Physically motivated, & Mid-$z$ Disfavoring & 
		Single, Broad Gaussians\\
		& featured $N(z)$ &&\\
		Surveyed & Unknown & \citet{Sheldon2012} & pseudo-random sample of BOSS DR10\\
		Biased & Unknown & \citet{Sheldon2012} & brightest 50\% of BOSS test
	\end{tabular}
	\caption{This table summarizes the validation tests using the shorthand names by which the tests are referenced.}
	\tablabel{tab:key}
\end{table}

\textul{Plots: flow chart of forward model}

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{figures/chippr/flowchart.png}
		\caption{In appendix?}
		\figlabel{fig:flowchart}
	\end{center}
\end{figure*}

%\clearpage
\subsubsection{Fiducial Data}
\sectlabel{sec:mock}

The following outlines the ``fiducial'' test case.  
We begin by choosing a physically-motivated $p^{*}(z)$ defining the general shape of $N(z)$ over the specified redshift range $[z_{min},z_{max}]$.  
Here we shall set it to a weighted sum of truncated Gaussians chosen to impose recognizably recoverable features on the true $N(z)$, shown in the top panel of \Fig{fig:physpz}.  
In the test case of \Sect{sec:fake} it shall be set to a narrow Gaussian shown in the bottom panel of \Fig{fig:physpz}.  
The true survey size $J$ is a Poisson random variable distributed about a target survey size of $J'$.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig2_physPz.pdf}\\
	\includegraphics[width=0.5\textwidth]{figures/chippr/delt_physPz.pdf}
	\caption{The physically-motivated true redshift distribution used in the mock data tests is chosen to have recognizable features of different scales so the quality of recovery can be easily determined.  
		The top panel shows a physically motivated true $N(z)$ for the fiducial test, and the bottom panel shows a toy model of a highly featured true $N(z)$.}
	\figlabel{fig:physpz}
\end{figure}

The catalog of $J$ photo-$z$ likelihoods $p(\vec{d}_{j}|z_{j})$ is chosen to be a set of Gaussian distributions, truncated to the redshift range over which $N(z)$ is defined, because it is the simplest extension of the standard redshift point estimates with reported error bars.  
To obtain the standard deviations $\sigma_{j}\sim\mathcal{N}(g\bar{\Delta},(g\bar{\Delta})^{2})$ associated with each surveyed galaxy $j$, we first sample a Gaussian 
distribution with mean and standard deviation equal to $g\bar{\Delta}$ for some factor $g$ (by default set to 2), truncated to enforce positive standard deviations.  
Each Gaussian is centered at an "observed" redshift $z'_{j}\sim\mathcal{N}(z^{*}_{j},\sigma_{j})$ separated from the true redshift $z^{*}_{j}$ by a Gaussian random variable $\epsilon_{j}\sim\mathcal{N}(0,\sigma^{2}_{j})$ selected from a distribution of mean of 0 and true standard deviation $\sigma_{j}$.   
This prescription may be understood as an exposition of the generative model of 
\begin{equation}
\eqlabel{eq:genmod}
z'_{j} = z^{*}_{j}+\epsilon_{j}
\end{equation}
for the physically-motivated process originating the data, which states that the MLE $z'_{j}$ of the redshift is equal to the true redshift $z^{*}_{j}$ plus a Gaussian random variable $\epsilon_{j}$.  
\Fig{fig:nullpzsgen} illustrates this procedure by showing the probability distribution $p(z'_{j}|z^{*}_{j})$ for a random galaxy $j$.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig2_zobsvztru.pdf}
	\caption{Top panel: This plot depicts $p(z'_{j}|z^{*}_{j})$ for a randomly chosen galaxy $j$ with areas of high probability in darker grayscale shades.  
		The colored dots represent coordinates at which the function is evaluated to produce the horizontal slices shown in the bottom panel.  
		Bottom panel: In colors are plotted several examples of individual photo-$z$ posteriors in the fiducial case with their MLE redshifts (dashed lines) and true redshifts (solid lines); these represent horizontal slices through the top panel.}
	\figlabel{fig:nullpzsgen}
\end{figure}

We must also choose the parametrization of the redshift distribution function, in this case that of \Eq{eq:steps}.   
We emphasize that the choice of parametrization here is arbitrary, but it is a choice that must at some point be made.  
By default we choose a flat distribution as the interim prior, but others will be tested.  
(Such a comparison has been executed before by \citet{Viironen2015}.)

To obtain the $J$ desired interim photo-$z$ posteriors $p(z_{j}|\vec{d}_{j})$ from the photo-$z$ likelihoods $p(\vec{d}_{j}|z_{j})$, we simply apply Bayes' rule as in 
\begin{equation}
\eqlabel{eq:likpost}
p(z_{j}|\vec{d}_{j}) \propto p(\vec{d}_{j}|z_{j})\ p(z_{j}|\vec{\theta}^{*}),
\end{equation}
A random sampling of such interim redshift posteriors is shown in \Fig{fig:nullpzs}.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig2_samplepzs.pdf}
	\caption{In colors are plotted several examples of individual photo-$z$ posteriors in the fiducial case with their MLE redshifts (dashed lines) and true redshifts (solid lines).}
	\figlabel{fig:nullpzs}
\end{figure}

\subsubsection{Intrinsic Scatter}
\sectlabel{sec:imprecision}

Several factors contribute to photometric redshifts' intrinsic scatter.  
Distant galaxies are dimmer compared to galaxies of identical luminosity that are closer, driving up photometric errors in flux-limited surveys.  
The nature of the galaxy sample at higher redshifts also changes, meaning the generation of the photometric redshift posterior based on an a locally-calibrated SED template library or spectroscopically-confirmed training set is more likely to be inappropriate, leading to broader features.  
In general, the galaxies that could not have been observed spectroscopically will have different and noisier photo-$z$ likelihoods than those that could fall into a spectroscopic training set (or spectroscopically derived template library).  
This effect may be stronger for high-redshift galaxies.  
This set of tests aims to demonstrate the performance of the sampler in cases with less and more random noise (via the factor $g$) and with noise that increases with redshift.

The first two test cases deviate from the fiducial case by varying the factor $g$ to be lower ($g=1$) and higher ($g=4$).  
Examples of interim photo-$z$ posteriors are plotted in \Fig{fig:sigspzs}.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig1_samplepzs.pdf}\\
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig4_samplepzs.pdf}
	\caption{As in \Fig{fig:nullpzs}, randomly sampled interim photo-$z$ posteriors are plotted in colors, along with the true redshift (solid line) and the maximum a posteriori redshift (dashed line).  
		Samples are shown for the precise case (top panel) and the imprecise case (bottom panel).}
	\figlabel{fig:sigspzs}
\end{figure}

The next test case deviates from the fiducial case by increasing the standard deviation of the photo-$z$ likelihood linearly with redshift. 
An illustration of the data generation process analogous to \Fig{fig:nullpzs} is shown in \Fig{fig:varspzs}.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/vars_zobsvztru.pdf}
	\caption{Same as \Fig{fig:nullpzsgen} but with standard deviation increasing linearly with true redshift.}
	\figlabel{fig:varspzs}
\end{figure}

\subsubsection{Catastrophic Outlier}
\sectlabel{sec:inaccuracy}

In this test, we aim to simulate more realistic interim photo-$z$ posteriors by modifying the procedure of \Sect{sec:mock} to introduce inaccuracy that causes catastrophic photo-$z$ errors.  
Catastrophic photo-$z$ errors arise from a degeneracy in the space of galaxy SEDs and redshifts, wherein a galaxy of one SED type at one redshift has photometry indistinguishable from a galaxy of another SED type at another redshift.  
In this case, there may be multiple peaks in the interim photo-$z$ posteriors.  

Catastrophic errors are simulated by considering multimodal photo-$z$ likelihoods.  
The first way to achieve this goal is to take the likelihoods to be sums of Gaussians of the form of those tested in \Sect{sec:null}.  
$R_{max}$ degenerate variances $\sigma_{R}^{2} $ and redshifts $z_{R}$ are chosen randomly for the entire survey.  
Each galaxy is assigned a number $R_{j}$ of Gaussian elements to be summed, chosen randomly from $R=1,\dots,R_{max}$ with weights proportional to $\sigma_{R}^{2}$.  
One $\sigma_{jr}$ is drawn for each $r=1,\dots,R_{j}$, and one $z'_{jr}$ is selected from the corresponding $\mathcal{N}(z_{R},\sigma^{2}_{jr})$.  
The components are summed and normalized to yield the likelihood, which is then convolved with the interim prior to produce a true interim posterior.   
An illustration of this method is shown in \Fig{fig:multpzsgen}. 

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/mult_zobsvztru.pdf}
	\caption{As in \Fig{fig:nullpzsgen} but for multimodal likelihoods.  
		Top panel: This plot depicts $p(z'_{j}|z^{*}_{j})$ for a randomly chosen galaxy $j$ with areas of high probability in darker grayscale shades.  
		The colored dots represent coordinates at which the function is evaluated to produce the likelihoods shown in the bottom panel.  
		Bottom panel: In colors are plotted several examples of individual photo-$z$ posteriors in the fiducial case with their peak redshifts (dashed lines) and true redshifts (solid lines).}
	\figlabel{fig:multpzsgen}
\end{figure}

A third way to produce multimodal or asymmetric posteriors is to sample the simple Gaussians of the fiducial case at the level of the posteriors.  
Here $K$ samples are taken.  
Some examples of both types of multimodal interim photo-$z$ posteriors are shown in \Fig{fig:allpzs}.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/mult_samplepzs.pdf}\\
	\includegraphics[width=0.5\textwidth]{figures/chippr/samp_samplepzs.pdf}
	\caption{Same as \Fig{fig:nullpzs} but for multiple likelihood components (top panel) and sampled posteriors (bottom panel).}
	\figlabel{fig:allpzs}
\end{figure}

\subsubsection{Toy Model}
\sectlabel{sec:fake-data}

We test the sampler in a case of a highly unrealistic but strongly featured true $N(z)$, that of the lower panel of \Fig{fig:physpz}.  
This is done to show that the sampler works even in extreme and unanticipated conditions.  
Instead of sampling the physically motivated true distribution $p(z|\vec{\theta}')$ as in \Sect{sec:mock}, we assign all galaxies the same true redshift.  

\subsubsection{Variable Interim Prior}
\sectlabel{sec:interim-data}

In the following two cases we vary the interim prior used in \Sect{sec:mock} to show that the sampler is robust to inappropriate choices of interim prior so long as that interim prior is known.  
Typically, interim redshift posteriors are made with an interim prior derived from $N(z)$ in a previous observational study.  
Since most observational studies used for this purpose are spectroscopically confirmed and objects for which photometric redshifts are relied upon make up a population that cannot be spectroscopically confirmed, such an interim prior is rarely appropriate.  
Some efforts have been made to modify an observationally informed interim prior so that it is more representative of the data set \citep{Sheldon2012}.  
However, any interim prior of this kind imparts information into the interim redshift posteriors.  
Ideally, an uninformative interim prior would be used, although it may be complicated to compute from the covariances of the raw data.  
In this test, we consider two obviously inappropriate interim priors and compare the result to that of the flat interim prior used in previous tests according to \Sect{sec:mock}.

In some cases, the interim prior is chosen to be the final product of a previously conducted spectroscopic redshift survey.  
Because low-redshift galaxies are more likely to be bright enough to be observed by such a survey, $N(z)$ determined from that sample may be heavily biased to low redshift galaxies.  
By contrast, the galaxies that were unobserved in such a survey are more likely be dimmer, making them more likely to be at higher redshifts.  
Since the interim prior is not compatible with our beliefs about the true redshift distribution, the resulting interim redshift posteriors will be inappropriate.  
In this test, we choose an interim prior with most of its weight at low redshifts and observe its influence on the recovery of the true $N(z)$ by different methods.  

Another potential method for selecting an interim prior with support over the entire redshift range expected of the photometric survey is to sum two or more $N(z)$ distributions obtained from reliable photometric surveys in the past.  
This is just as problematic as using a biased spectroscopically derived $N(z)$ as the interim prior because the sum of redshift distributions for two or more surveys does not reflect our beliefs about the true distribution for a single survey even though it provides support over the same redshift range.  
To simulate this case, we choose an interim prior with more weight at high and low redshifts than for mid-range redshifts.  

%\clearpage
\subsubsection{BOSS Data}
\sectlabel{sec:data}

We also test this method on subsets of the published interim photo-$z$ posteriors of SDSS III DR 10. 
A sampling of the provided interim photo-$z$ posteriors of dimension $K=35$ for $z_{min}=0.3$ and $z_{max}=1.4$ is shown in the top panel of \Fig{fig:datapzs}.  
The brightest half of this pseudo-random sampling are selected for another test, with photo-$z$ interim posterior samples shown in the bottom panel of \Fig{fig:datapzs}.  
The interim prior used for this set of interim photo-$z$ posteriors is the reweighted estimator of $N(z)$ of \citet{Sheldon2012}.  
In these cases the true $N(z)$ is not known, but the fully probabilistic method presented here may still be compared to what is obtained by alternative approaches.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/boss_samplepzs.pdf}\\
	\includegraphics[width=0.5\textwidth]{figures/chippr/bias_samplepzs.pdf}
	\caption{Same as \Fig{fig:nullpzs} but for real data.  
		A pseudo-random sampling of BOSS DR 10 photo-$z$ posteriors produced by \citet{Sheldon2012} (top panel) are clearly much noisier than those of the fiducial case.  
		A random sampling of the brightest half of the pseudo-random BOSS DR10 sample (bottom panel) corresponds to much cleaner photo-$z$ posteriors.}
	\figlabel{fig:datapzs}
\end{figure}

%\clearpage
\section{Validation Tests}
\sectlabel{sec:valid}

Here we present the results of the informative tests of \Sect{sec:exp}, summarized in \Tab{tab:key}.  

\subsection{Fiducial Case}
\sectlabel{sec:null}

\Fig{fig:null-samp} shows a selection of random samples from the posterior distribution of \Eq{eq:final} for the fiducial setup of \Sect{sec:mock}.  
\Fig{fig:null-comp} compares the mean of the sample values to alternative estimators as well as the truth.  
In the fiducial case, hierarchical inference preserves features in the redshift distribution function, whereas stacking tends to smear it out.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/null_samps.pdf}
	\caption{This test was conducted with a flat interim prior (gray line) and produced a mean of samples (thick, black line) that accurately reproduces the truth.  
		It can be seen that the samples (colored lines) are an excellent estimator of the true $N(z)$ (thin, black line) with the true values falling within the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray).}
	\label{fig:null-samp}
\end{figure}

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/null_comps.pdf}
	\caption{The mean of the samples (thick, solid line) and marginalized MLE (thick, dotted line) are the best estimators of the true distribution (thin, solid line), and the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) appear to be accurate.  
		Stacking (thick, dashed line) overestimates $N(z)$ at the tails and underestimates $N(z)$ at the peaks.  
		The two point estimators considered here, marginalized MAP (thin, dashed line) and marginalized expected value (thin, dotted line) fall between these two 
		extremes.}
	\figlabel{fig:null-comp}
\end{figure}

%\clearpage
\subsection{Intrinsic Scatter}
\sectlabel{sec:noisy}

The sampler was run on the two datasets described in \Sect{sec:imprecision}.  
In the case of less noisy data (\Fig{fig:sig1-comp}), the error bars were smaller than in the fiducial case, and in the case of noisier data (\Fig{fig:sig4-comp}), the error bars were larger than in the fiducial case.  
However, in both these cases, the value of the mean of the samples was virtually unaffected.  
Even in the best case of $g=1$, the alternative methods are still negatively affected; in the case of $g=4$, it can be plainly seen that the marginalized maximum a posteriori estimator experiences instability at the edges of the distribution and the other alternatives estimate a substantially broader distribution than the accurate sampler.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig1_comps.pdf}
	\caption{Here it is shown that the alternative methods erase features in estimating the true $N(z)$ (thin, solid line) even in the case of optimistically precise photo-$z$ interim posteriors.  
		Stacking (thick, dashed line), the marginalized maximum a posteriori estimator (thin, dashed line), and the marginalized expected value (thin, dotted line) all smooth the features of the distribution, an effect that is reflected in the KLD.  
		The mean of posterior samples (thick, solid line) and marginalized MMLE (thick, dotted line) perform better than the alternatives, with the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) on the mean of samples being appropriately shrunken.}
	\figlabel{fig:sig1-comp}
\end{figure}

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/sig4_comps.pdf}
	\caption{The alternative estimators erase features in estimating the true $N(z)$ (thin, solid line) more severely for noisier interim photo-$z$ posteriors.  
		The marginalized maximum a posteriori estimator (thin, dashed line), stacked estimator (thick, dashed line), and marginalized expected value (thin, dotted line) estimate overly smooth redshift distributions.  
		The mean of posterior samples (thick, solid line) and marginalized maximum likelihood estimator (thick, dotted line) perform far better than the alternatives, with the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) on the posterior samples being appropriately inflated.}
	\figlabel{fig:sig4-comp}
\end{figure}

In the third case simulating intrinsic scatter, in which the width of the Gaussian likelihoods increases with redshift, the sampler again is a better estimator of the true redshift distribution.  
In \Fig{fig:vars-comp}, one can see that systematics are present in all point estimators, including that of the mean of posterior samples.  
However the distribution of posterior samples is broader in regions where the mean is farther from the truth, whereas the point estimators cannot accurately estimate error bars.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/vars_comps.pdf}
	\caption{The mean of sampled values (thick, solid line) recovers the general features of the true $N(z)$ (thin, solid line), with larger error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) in regions where it deviates more from the truth.  
		The marginalized maximum a posteriori estimator (thin, dashed line) follows the same skewed yet featureless distribution of the stacked estimator (thick, dashed line) but with an overestimate of probability at the edges of the distribution.  
		The marginalized expected value estimator (thin, dotted line) is truly featureless without even the skew reflecting the increased uncertainties at higher redshift.  
		The marginalized maximum likelihood estimator (thick, dotted line) performs comparably to the mean of the posterior samples, modulo some numerical instability.}
	\figlabel{fig:vars-comp}
\end{figure}

From these three test cases, it can be seen that the mean of the sampled hyperparameter values is a robust estimator of the true $N(z)$, regardless of the imprecision of the data.  
The width of the posterior distribution over hyperparameters accurately reflects our beliefs about the uncertainty of this estimator.  
Stacking and the other point estimators generally fail to recover the distinctive features of the true $N(z)$.

%\clearpage
\subsection{Inaccurate interim photo-$z$ posteriors}
\sectlabel{sec:multi}

The effect of catastrophic outliers on hyperparameter estimation, simulated according to the procedure of \Sect{sec:inaccuracy} is described here.  

For the case of multimodal likelihoods, \Fig{fig:multi-comp} shows a comparison of the mean of the samples to alternative methods.  
It can be seen that the lessened quality of the data does not impact the accuracy of the mean of the posterior samples as an estimator of the truth.  
However, both the error bars on the mean of posterior samples and the alternative point estimators are worse in the sense of being broader.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/mult_comps.pdf}
	\caption{Both the mean of posterior samples (thick, solid line) and marginalized maximum likelihood estimator (thick, dotted line) do a good job of recovering the true redshift distribution function (thin, solid line).  
		The stacked estimator (thick, dashed line), marginalized maximum a posteriori estimator (thin, dashed line), and marginalized expected value estimator (thick, dotted line) are smoother and fail to recover the features of the true redshift distribution.  
		The error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) on the mean of posterior samples include the true value of the hyperparameters.}
	\figlabel{fig:multi-comp}
\end{figure}

For the case of sampled posteriors, \Fig{fig:samp-comp} shows a comparison of the mean of the samples to alternative methods.  
It can be seen that the lessened quality of the data does impart some systematic effects onto the mean of samples that is not seen in the marginalized maximum likelihood estimator, but the error bars are broader as well to reflect the uncertainty of the mean of posterior samples.  
The alternative point estimators still fail to recover features of the true redshift distribution.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/samp_comps.pdf}
	\caption{In the case of sampled interim photo-$z$ posteriors, the mean of posterior samples (thick, solid line) and marginalized maximum likelihood estimator (thick, dotted line) do a good job of recovering the true redshift distribution function (thin, solid line), though the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) of the posterior samples appear to be underestimated.  
		The stacked estimator (thick, dashed line), marginalized maximum a posteriori estimator (thin, dashed line), and marginalized expected value estimator (thick, dotted line) are smoother and fail to recover the features of the true redshift distribution.}
	\figlabel{fig:samp-comp}
\end{figure}

%\clearpage
\subsection{Toy Model $N(z)$}
\sectlabel{sec:fake}

\Fig{fig:toy-comp} compares the mean of the posterior samples to the results of stacking and marginalized likelihood maximization.  
It can be seen that the marginalized maximum likelihood estimator is best at recovering the true distribution approaching a delta function due to the nature of the optimizer, which considers each component of the hyperparameter vector to be independent of all others.  
Other estimators predict a broader distribution than the truth, with stacking broadening it the most and the mean of the samples broadening it the least.  
Though the sampler does not consider components of the hyperparameter vector to be independent, it is flexible enough to provide good constraints on the true $N(z)$.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/delt_comps.pdf}
	\caption{In this case, the marginalized maximum likelihood estimator (thick, dotted line) recovers the true $N(z)$ (thin, solid line) better than the mean of posterior samples (thick, solid line), though the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) do include the true values of the hyperparameters. 
		The stacked estimator (thick, dashed line) is the worst estimator of the true redshift distribution function, while the marginalized maximum a posteriori estimator (thin, dashed line) and marginalized expected value estimator (thin, dotted line) are not quite as broad.}
	\figlabel{fig:toy-comp}
\end{figure}

%\clearpage
\subsection{Variable Interim Prior}
\sectlabel{sec:interim}

The cases of inappropriate interim priors also demonstrate the prowess of the sampling approach.  
The result for a unimodal, low-$z$-favoring interim prior is shown in \Fig{fig:intu-comp}, and the result for a bimodal, mid-$z$-disfavoring interim prior is shown in \Fig{fig:intb-comp}.  
The methods that account for the nontrivial interim prior (the marginalized maximum likelihood estimator and the sampler) yield substantially less biased results than the methods that do not, and those methods are strongly biased towards the interim prior distribution.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/uint_comps.pdf}
	\caption{The mean of the sampled values (thick, solid line) and marginalized maximum likelihood estimator (thick, dotted line) are unaffected by the nontrivial interim prior, with appropriate error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) on the samples from the posterior distribution of hyperparameters.  
		However, the alternatives yield biased estimates, both smoothing the features of the true $N(z)$ (thin, solid line) and showing a bias to low redshift values favored by the interim prior (gray line).  
		Stacking (thick, dashed line) does the worst of the three alternatives, with the marginalized maximum a posteriori estimator (thin, dashed line) and marginalized expected value (thin, dotted line) doing slightly better.}
	\figlabel{fig:intu-comp}
\end{figure}

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/bint_comps.pdf}
	\caption{\textbf{I am redoing this test with the interim prior tweaked to provide full coverage over the whole redshift range to eliminate the 
			underestimate in the last bin}.  
			The mean of the sampled values (thick, solid line) and marginalized maximum likelihood estimator (thick, dotted line) are unaffected by the nontrivial interim prior, with appropriate error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) on the samples from the posterior distribution of hyperparameters.  
			However, the alternatives of the stacked estimator (thick, dashed line), the marginalized maximum a posteriori estimator (thin, dashed line), and marginalized expected value (thin, dotted line) yielding biased estimates, both smoothing the features of the true $N(z)$ (thin, solid line) and showing a bias to low and high redshift values favored by the interim prior (gray line).}
	\figlabel{fig:intb-comp}
\end{figure}

%\clearpage
\subsection{Real Data}
\sectlabel{sec:boss}

The results of the inference of the redshift distribution function from a pseudo-random sample of BOSS DR10 data described in \Sect{sec:data} are shown in \Fig{fig:dataparam} and \Fig{fig:datacomp}.  

The most striking feature of \Fig{fig:dataparam} aside from the stark difference between the mean of the samples and the interim prior is the major systematic in the samples from the posterior distribution is observed at high redshift.  
Because the data itself exhibits high uncertainty at high $z$ in the form of local maxima of the photo-$z$ interim posteriors, a reflection of the inherent degeneracies that give rise to catastrophic photo-$z$ errors, it is natural that there be large errors in that region.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/boss_samps.pdf}
	\caption{Samples from the full posterior (colored lines) of the subsample differ substantially from the interim prior (gray).  
		The mean of samples (thick, black line) and associated error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) are in line with the sampled values.}
	\figlabel{fig:dataparam}
\end{figure}

\Fig{fig:datacomp} also shows the broader error bars in a small region of high redshift, but it has the additional information of how other estimators perform.  
The results of stacking and reduction of interim photo-$z$ posteriors to point estimators are strongly biased toward the interim prior, especially stacking.  
Given that stacking has been validated by the results' similarity to the interim prior, it is especially grave that it trivially reproduce it; if the interim prior is inappropriate, stacking is guaranteed to fail because it do not account for the effect of the interim prior on the interim photo-$z$ posteriors, permitting it to dominate over the underling likelihoods.  
The result of marginalized maximum likelihood estimation is also shown to be numerically unstable, a possible effect of the extreme multimodality of the data.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/boss_comps.pdf}
	\caption{It can be seen that the stacked estimator (thick, dashed line) almost perfectly reproduces the interim prior (gray line), while the marginalized maximum a posteriori estimator (thin, dashed line) and marginalized expected value estimator (thin, dotted line) do so with some instability.  
		The marginalized maximum likelihood estimator (thick, dotted line) is most unstable, while the mean of the posterior samples (thick, solid line) predicts a very different redshift distribution function than the interim prior, with a peculiar feature in the error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) at high redshift.}
	\figlabel{fig:datacomp}
\end{figure}

The BOSS subsample is further subsampled by imposing a cut in $r$-band magnitude (at the median magnitude) to approximate the behavior of a heavily biased galaxy survey with a magnitude limit.  
This is motivated by the question of what data is behind the feature in the posterior distribution's error bars at high redshift.  
Recall that samples of photo-$z$ interim posteriors are shown in the lower panel of \Fig{fig:datapzs}.  
The results of the inference are shown in \Fig{fig:biasparam} and \Fig{fig:biascomp}.  

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/bias_samps.pdf}
	\caption{Samples from the full posterior (colored lines) of the biased subsample still differ substantially from the interim prior (gray).  
		The mean of samples (thick, black line) and associated error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) are in line with the sampled values.}
	\figlabel{fig:biasparam}
\end{figure}

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figures/chippr/bias_comps.pdf}
	\caption{The mean of posterior samples (thick, solid line) differs from the interim prior (gray line) more than the alternative estimators, exhibiting some features at low redshift.  
		It can be seen that the stacked estimator (thick, dashed line) is most biased towards the interim prior, though the effect is also seen in the marginalized maximum a posteriori estimator (thin, dashed line) and marginalized expectation value estimator (thin, dotted line).  
		The marginalized MLE (thick, dotted line) is still highly unstable.  
		The error bars ($1\sigma$ in dark gray, $2\sigma$ in light gray) do not exhibit the same behavior as in the unbiased subsample.}
	\figlabel{fig:biascomp}
\end{figure}

There are two notable differences between the unbiased and biased subsamples of the BOSS DR10 pseudo-random subsample.  
First, the stacked and point estimators are less biased toward the interim prior.  
This may be due to the way the interim prior was chosen, such that it was influenced strongly by the dimmest galaxies.  
Second, the high-redshift feature in the error bars is not present once a magnitude cut is imposed.  
From this one can conclude that the signal and associated error bars in that region were caused by the sampler's assignment high redshifts to poorly constrained interim photo-$z$ posteriors corresponding to the dimmest galaxies. 

\section{Results and discussion}
\sectlabel{sec:results}

\aim{Q: How does the result of \chippr\ compare to established estimators in terms of the accuracy of $n(z)$?\\
	A: \chippr\ yields the best possible $n(z)$, conditional on the accuracy of the photo-$z$ PDFs used.}

\subsection{LSST Requirements}
\sectlabel{sec:lsstdemo}

\aim{LSST specs: biased to 0.003, 10\% uniform outliers, z-dependent scatter of 0.05}

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/chippr/lsst_scatter.png}
		\includegraphics[width=0.45\textwidth]{figures/chippr/lsst_log_estimators.png}
		\caption{Using LSST numbers}
		\figlabel{fig:lsstdemo}
	\end{center}
\end{figure*}

\subsection{Violations of the model}
\sectlabel{sec:violations}

\aim{mischaracterized interim prior, with idealized or realistic data?}

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/chippr/wrong_scatter.png}
		\includegraphics[width=0.45\textwidth]{figures/chippr/wrong_log_estimators.png}
		\caption{this is for wrong implicit prior, not bias, haven't done that one yet}
		\figlabel{fig:mischaracterized}
	\end{center}
\end{figure*} 

\subsection{Discussion}
\sectlabel{sec:discussion}

\begin{figure*}
	\begin{center}
		\includegraphics[width=0.99\textwidth]{figures/chippr/final_plot.png}
		\caption{propagation to cosmology}
		\figlabel{fig:cornerplot}
	\end{center}
\end{figure*}

\section{Conclusion}
\sectlabel{sec:con}

This study derives and demonstrates a mathematically consistent implementation of inference of a one-point statistic based on interim photo-$z$ posteriors.  
The fully Bayesian method, based in the fundamental laws of probability, begins with a graphical model corresponding to equations for the full posterior.  
The technique developed in this paper is applied to the example of the redshift distribution function $N(z)$ with promising results on mock data; not only is this the only mathematically correct approach to the problem, it also recovers the true parameter values better than popular alternatives.  

In the tests on simulated data performed here, the full posterior distribution over the hyperparameters defining $N(z)$ derived by this method is consistent with the true redshift distribution function, making the mean of sampled values an excellent point estimator of $N(z)$.  
The information contained in the full posterior distribution's shape convey the traditional error bar information without having to explicitly propagate any error estimates.  
The results of those tests is summarized below and in Tab. \ref{tab:kld}, where lower values indicate a closer match between the true $N(z)$ and the estimator.  
Tests were also performed on subsets of BOSS DR10 data with results consistent with those of simulations.

The following conclusions and recommendations can be made with confidence:

\begin{enumerate}
	\item Both the marginalized maximum likelihood estimator and the mean of the samples are good point estimators of the redshift distribution function; the error bars on the posterior distribution over hyperparameters are generally reliable and easier to derive than error bars from traditional point estimators of the redshift distribution function.
	\item Even in the case of precise data, traditional methods fail, and they do even worse with imprecise data; the error bars of the mean of posterior samples, however, are accurate.  When data quality worsens with redshift, all estimators are affected, but the inaccuracy is quantified by the full distribution of the posterior samples whereas the alternative estimators require separate error propagation.  
	\item When data suffers from inaccuracies, as in the case of multimodal interim photo-$z$ posteriors, the marginalized maximum likelihood estimator performs comparably to the mean of the posterior samples
	\item The marginalized maximum likelihood estimator is an excellent estimator for strongly featured redshift distribution function with simple, clean photo-$z$ posteriors; stacking smooths features more than sampling and photo-$z$ point estimation.
	\item When the interim prior is known to be a poor approximation to the data, only the marginalized MLE and mean of sampled values are satisfactory estimators of the redshift distribution function because they are the only methods that can account for the bias introduced into the photo-$z$ posteriors; this is the most compelling case for the sampler because of the ubiquity of inappropriate interim priors.
\end{enumerate}

\begin{table}
	\begin{tabular}{lccccc}
		& Mean of & Marginalized & Stacked & Marginalized & Marginalized\\
		& Samples & MLE & Estimator & MAP & Expected Value\\
		Fiducial &\textbf{0.0041}&0.0057&0.0139&0.0098&0.0072\\
		Precise &\textbf{0.0035}&0.0065&0.0176&0.0112&0.0102\\
		Imprecise &\textbf{0.0068}&0.0155&0.1014&0.1207&0.1028\\
		Trending &\textbf{0.0199}&0.0262&0.1057&0.1288&0.0813\\
		Multimodal &\textbf{0.0052}&0.0054&0.0508&0.0385&0.0250\\
		Sampled &0.0189&\textbf{0.0026}&0.0475&0.0446&0.0230\\
		Featured &0.0104&\textbf{0.0060}&0.2902&0.1721&0.1720\\
		Unimodal &\textbf{0.0030}&0.0046&0.0150&0.0128&0.0127\\
		Bimodal &\textbf{0.0037}&0.0056&0.0143&0.0122&0.0141
	\end{tabular}
	\caption{The KLD values for all estimators in each simulated test are provided here.
		The best-fit estimator in each case is bolded.}
	\tablabel{tab:kld}
\end{table}

By showing that this method is effective in recovering the true redshift distribution function and posterior distributions on its parameters from simulated interim photo-$z$ posteriors, this work supports the production of interim photo-$z$ posteriors by upcoming photometric surveys such as LSST so that more accurate inference of physical parameters may be accessible to the scientific community.  
We discourage researchers from co-adding interim photo-$z$ posteriors or converting them into point estimates of redshift and instead recommend the use of Bayesian probability to guide the usage of interim photo-$z$ posteriors in science.  
We emphasize to those who produce interim photo-$z$ posteriors from data that it is essential to release the interim prior used in generating this data product in order for proper inference to be conducted by consumers of this information.

The technique herein developed is applicable with minimal modification to other one-point statistics of redshift to which we will apply this method in the future, such as the redshift-dependent luminosity function and weak lensing mean distance ratio.  
Future work will also include the extension of this fully probabilistic approach to higher-order statistics of redshift such as the two-point correlation function.

\textbf{What else should I be putting here?  Is this a good place to put the discussion of how different estimators of $N(z)$ affect different estimators of the weak lensing power spectrum?  That analysis has been completed, but I'm not sure where it fits into the paper.}

\section{Chapter acknowledgements}

AIM thanks Elisabeth Krause for assistance with the \texttt{CosmoLike} code, Mohammadjavad Vakili for insightful input on statistics, Geoffrey Ryan for advice on debugging, and Boris Leistedt for helpful comments provided in the preparation of this paper.
I thank Phil Marshall, David Hogg, Rachel Mandelbaum, Eduardo Rozo, David Alonso, Daniel Gruen, Boris Leistedt, Josh Speagle, and Mark Manera for helpful feedback provided in the preparation of this letter.
This work was completed under the generous nutritional support of the Center for Computational Astrophysics.
