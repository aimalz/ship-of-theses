\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}

[Why do we need redshifts? cosmology problems and galaxy evolution, use as distance proxy via Hubble law]

Photometric redshift (\pz) estimation has been a staple of studies of galaxy evolution, large-scale structure, and cosmology since its conception decades ago \citep{Baum1962}.  
An extremely coarse spectrum in the form of photometry in a handful of broadband filters can be an effective substitute for the time-intensive process of obtaining a spectroscopic redshift (\sz), a procedure that may only be applied to relatively bright galaxies.  
Once the photometric colors are calibrated against either a library of spectral energy distribution (SED) templates or a data set of spectra for galaxies with known redshifts, a correspondence between photometric colors and redshifts may be constructed, forming a trustworthy basis for \pz\ estimation or testing.

Many more \pz s may be obtained in the time it would take to observe a smaller number of \sz s, and \pz s may be measured for galaxies too dim for accurate \sz\ confirmation, permitting the compilation of large surveys of galaxy redshifts spanning a broad range of redshifts and luminosities.  
\Pz s have thus enabled the era of precision cosmology, heralded by weak gravitational lensing tomography and baryon acoustic oscillation peak measurements.  
Calculations of correlation functions of cosmic shears and galaxy positions require large numbers of high-confidence redshifts of surveyed galaxies.  

However, \pz s are susceptible to a number of sources of error, particularly their inherent noisiness due to the coarseness of photometric filters, catastrophic errors in which galaxies of one type at one redshift are mistaken for galaxies of another type at a different redshift, and systematics introduced by observational techniques, data reduction processes, and training set limitations.  
In addition to these limitations in accuracy, there is also the matter of precision; \pz s are often reported with error bars derived without inclusion of all systematic errors, including the different selection effects between the photometric color- or magnitude-spaces of galaxies for which \pz s are desired and galaxies with \sz s are used to calibrate \pz\ estimators.

Once propagated through the calculations of correlation functions of cosmic shear and galaxy positions, these sources of \pz\ errors are not insignificant contributors to the total uncertainties reported on cosmological parameters; as other systematic errors have been resolved, the uncertainties associated with \pz s have come to dominate the uncertainties on estimates of cosmological parameters made by current surveys such as DES.

Much effort has been dedicated to improving \pz s, though they are still most commonly obtained by a maximum likelihood estimator (MLE) based on libraries of galaxy SED templates, with conservative approaches to error estimation.  
Recent advances have focused on identifying and removing catastrophic outliers when using \pz s for 
inference \citep{Gorecki2014}.  
Sophisticated Bayesian techniques and cutting-edge machine learning methods have been employed to improve precision \citep{Carliles2010} and accuracy \citep{Sadeh2015}. 

An alternative to point estimation of \pz s is redshift probability distribution function (PDF) estimation, in which rather than an MLE point estimate, the full posterior PDF of the redshift of a galaxy is reported \citep{Koo1999}.  
This option is favorable because it contains more potentially useful information than a point estimate while addressing the issues with precision, accuracy, and systematics.  

Many techniques to obtain photo-$z$ probability distributions have been proposed and tested in the literature.  
An extension of the Bayesian photometric redshift (BPZ) method of \citet{Benitez2000} that produces posterior probability distributions (as opposed to a selection of local maxima) from an SED template library has been employed \citep{Hildebrandt2012, Kelly2014, Lopez-Sanjuan2015}.  
\Pzpdf s have also been obtained by a variety of trustworthy data-driven approaches in the literature: $k$-nearest neighbor algorithms with \citep{Ball2008} and without \citep{Sheldon2012} inclusion of photometric measurement errors, neural networks \citep{Bonnett2015a}, self-organizing maps \citep{CarrascoKind2014a}, and prediction tree and random forest classification techniques \citep{Carliles2010, CarrascoKind2013}.  
(The approaches of fitting to a training set and fitting to a template library are related by \citet{Budavari2009}.)  
Hierarchical inference has also been applied to calculate \pzpdf s simultaneously with the overall redshift distribution function \citep{Leistedt2016}.  
Some current work aims to vet photo-$z$ probability distribution generation methods \citep{Wittman2016}, but much remains to be done.  
Of course, this brief review does not cover all ways to obtain \pzpdf s; many more may be found in the literature, along with comparisons thereof \citep{Hildebrandt2010, Dahlen2013, Sanchez2013, Bonnett2015}.

\Pzpdf s have been produced by completed surveys \citep{Hildebrandt2012, Sheldon2012} and will be produced by ongoing and upcoming surveys \citep{LSSTScienceCollaboration2009, CarrascoKind2014a, Bonnett2015, Masters2015}.  
\Pzpdf s are not without their own weaknesses, however, including the resources necessary to calculate and record them for large galaxy surveys \citep{CarrascoKind2014} and the method used to derive them.  
The most important of these issues, however, is that use of them in the literature is inconsistent at best and incorrect at worst.  

\clearpage

\aim{[terms I should define up front: \pz s, probability density functions]}

what is stacking:
\begin{align}
\eqlabel{eqn:stacked}
\hat{n}(z) &\equiv \frac{1}{J}\ \sum_{j=1}^{J}\ \pr(z_{j})
\end{align}

\begin{itemize}
	\item \pzpdf\ a.k.a. ``$\pr(z)$'' is misleading
	\item \pzpdf s are posteriors conditioned on data $\pr(z \gvn \data_{i})$, must be posterior  and $\pr(z_{i})$ doesn't make sense because the domain of $z$ is the same for any galaxy $i$
	\item \pzpdf s are also conditioned on the information $I$ used to derive them, such as a template library or training set.
	\item there is also a prior due to the method used to combine $I$ and $\data_{i}$ to yield an estimated $\pr(z \gvn \data_{i}, I)$, which is an implicit prior, making \pzpdf s implicit posteriors $\pr(z \gvn \data_{i}, I)$
\end{itemize}

\aim{[what interesting problems/opportunities are the context for my work?  (drawing conclusions about the universe from vast quantities of uncertainty-dominated, limitations of photometry lead to methodological challenges, what was tried so far and why isn't it good enough)]}

\aim{[what science can be done with that opportunity? (cosmology, and then some! the interesting problems are in testing LCDM, GR, inflation, also galaxy evolution)]}

\aim{[what I'm going to say in each chapter, one paragraph each, pose the problem I solve and say what chapter contains my solution]}

\Chap{chippr} presents a mathematically rigorous methodology for inferring the redshift distribution function from a catalog of redshift posteriors.
\Chap{pzdc1} contains a comprehensive comparison of twelve approaches to probabilistic photometric redshift estimation, presenting novel discoveries of the impact of the assumptions implicit to the method by which the redshift probabilities are derived and the limitations of established performance metrics of such probabilistic data products in assessing the quality of the procedures for deriving them.
I also address in \Chap{qp} a practical concern regarding how redshift probabilities are to actually be used, answering the question of how probabilistic data products should be stored and delivered to users in order to ensure that scientific progress can be made, as well as how to go about answering that question for a generic science application.

[did I produce anything of value? if there's a product, maybe cite it here?]

\chapname~\chapalt{chippr} has been presented at numerous conferences but remains an unpublished, public draft accompanied by a public code.
\chapname~\chapalt{pzdc1} is currently under \desc\ internal review, with intended submission to MNRAS.
\chapname~\chapalt{qp} has been refereed and published in AJ, with code published on Zenodo.
Each \chapname\ was co-authored with collaborators but the writing in each \chapname\ is mine.
Here, I describe my specific contributions to each \chapname:
\begin{enumerate}

{\item For \chap{chippr}, I led the development, implementation, and validation of CHIPPR under the supervision of David Hogg (NYU) and developed the mathematical formalism with encouragement from Phil Marshall (SLAC).}

{\item For \chap{pzdc1}, I led the choice, evaluation, and interpretation of the comparison metrics, and I devised and implemented the \texttt{trainZ} technique of \pzpdf\ estimation.
I wrote the paper collaboratively with Sam Schmidt and the \Pz\ Working Group of the \desc.
The mock data was produced by [just a few people] and the \pzpdf s were calculated by [almost all the other people].}

{\item For \chap{qp}, I conducted the investigation and wrote the \texttt{qp} code under the supervision of Phil Marshall (SLAC), with contributions to the mock data production by Sam Schmidt (UC Davis), Melissa Graham (UW), Joe DeRose (Stanford), and Risa Wechsler (Stanford).}

\end{enumerate}
